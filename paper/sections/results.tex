The hyper parameters found to perform the best on the validation are a batch size of 32, a sample size (how many of the first words of a Wiki article summary are included) of 16, Adam as optimizer. Training on how to weight $w$ improved accuracy by 0.5. Inspection of $v$ seems to indicate that the msot informative articles are in the long tail.

The model achives an accuracy of 0.60, which with $N=120$ yield a $p$-value of 0.01.
Splitting by variacne and average (N=60) for each yields accuracies of 0.62 ($p$ 0.04) and 0.58 ($p$ 0.004).

Spitting by factors yields the following matrix.

With $N=24$ for each factor test no siginificant p-value is reached (bernfandino correction). 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|} 
\hline
factor & f1 & f2 & f3 & f4 & f5 \\ 
\hline
weight & 0.5 & 0.2 & 0.1 & 0.1 & 0.1 \\ 
\hline
average &  &  &  &  &   \\
\hline
variance &  &  &  &  &   \\
\hline
\end{tabular}
\end{table}

Though the $p$-values of the individual factors are below the significance threshold, whether the model is equally good at predicting onall five factors can be computed.
Assuming equal probability of being right within wach factor guess, are the observerd difference in accuracy unlikely.
With a $p$-value of 0.004 this is indeed found to be th case.



These resulsts can be replicated by runnning \texttt{docker run syrkis/bsc}.
